{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## E-Commerce Data Raw Ingestion Pipeline\n",
        "\n",
        "This notebook represents the first stage in the E-commerce ETL pipeline. It performs:\n",
        "1. Acquisition of raw data from the Kaggle Brazilian E-commerce dataset by Olist\n",
        "2. Secure transfer of data to Azure Data Lake Storage (ADLS)\n",
        "3. Preparation for subsequent transformation stages\n",
        "\n",
        "### Environment Setup and Credential Management\n",
        "\n",
        "The following code:\n",
        "1. Installs the Kaggle API for data acquisition\n",
        "2. Initializes the Spark session\n",
        "3. Securely retrieves Kaggle credentials from Azure Key Vault\n",
        "4. Configures the environment for Kaggle API authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Install Kaggle API (only needed once)\n",
        "!pip install kaggle\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Retrieve Kaggle API credentials from Azure Key Vault\n",
        "kaggle_credential = mssparkutils.credentials.getSecret(\"ecom-sales-kv\", \"KaggleAPI\")\n",
        "\n",
        "# Save Kaggle API credentials securely\n",
        "kaggle_json_path = \"/tmp/kaggle.json\"\n",
        "with open(kaggle_json_path, \"w\") as f:\n",
        "    f.write(kaggle_credential)\n",
        "\n",
        "# Set permissions and environment variable for Kaggle\n",
        "!chmod 600 /tmp/kaggle.json\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/tmp'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Dataset Acquisition\n",
        "\n",
        "This section:\n",
        "1. Downloads the Brazilian E-commerce dataset from Kaggle\n",
        "2. Extracts the compressed files to a temporary location\n",
        "3. Prepares the data for upload to the data lake\n",
        "\n",
        "The Olist dataset contains anonymized commercial data from multiple Brazilian marketplaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "download_path = \"/synfs/tmp\"\n",
        "os.system(f\"kaggle datasets download -d olistbr/brazilian-ecommerce --unzip -p {download_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Storage Path Configuration\n",
        "\n",
        "This section defines a utility function that:\n",
        "1. Standardizes path generation for different data lake containers\n",
        "2. Ensures consistent access patterns throughout the ETL pipeline\n",
        "3. Encapsulates storage account information for maintainability\n",
        "\n",
        "This modular approach simplifies storage path management across all pipeline stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Step 2: Define a function for generating ADLS paths\n",
        "def get_adls_path(container: str, folder: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate an ADLS path based on the container and folder.\n",
        "    \n",
        "    Parameters:\n",
        "        container (str): The ADLS container (e.g., 'raw', 'processed', 'curated').\n",
        "        folder (str): The folder path inside the container.\n",
        "    \n",
        "    Returns:\n",
        "        str: The formatted ADLS path.\n",
        "    \"\"\"\n",
        "    storage_account = \"ecomsalessa\"\n",
        "    return f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{folder}/\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Data Transfer to Azure Data Lake Storage\n",
        "\n",
        "This section:\n",
        "1. Transfers the downloaded files to the raw container in ADLS\n",
        "2. Maintains the original file structure and formats\n",
        "3. Prepares the data for transformation in subsequent pipeline stages\n",
        "\n",
        "This completes the raw data ingestion process, creating a reliable foundation for the ETL pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "adls_path = get_adls_path(\"raw\", \"ecommerce-dataset\")\n",
        "\n",
        "for file in os.listdir(download_path):\n",
        "    local_file_path = os.path.join(download_path, file)\n",
        "    adls_file_path = os.path.join(adls_path, file)\n",
        "\n",
        "    if os.path.isfile(local_file_path):\n",
        "        print(f\"Uploading {file} to ADLS...\")\n",
        "        mssparkutils.fs.cp(f\"file://{local_file_path}\", adls_file_path, recurse=True)\n",
        "\n",
        "print(\"Upload completed successfully.\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}