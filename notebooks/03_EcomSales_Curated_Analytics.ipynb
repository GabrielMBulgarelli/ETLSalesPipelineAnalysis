{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E-Commerce Data Curated Analytics \n",
        "### Dimensional Modeling & Aggregation\n",
        "\n",
        "This notebook represents the final stage in the E-commerce ETL pipeline. It:\n",
        "1. Creates a dimensional model (star schema) from processed data\n",
        "2. Builds business aggregations for analytics\n",
        "3. Performs data quality checks\n",
        "4. Writes curated data to the data lake for consumption\n",
        "\n",
        "### Configuration and Environment Setup\n",
        "\n",
        "The following cell initializes the Spark environment and defines storage paths for the ETL process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-21T11:42:20.9128253Z",
              "execution_start_time": "2025-03-21T11:42:20.6716597Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "6d9ea30c-4eda-4518-93f6-04c44f113289",
              "queued_time": "2025-03-21T11:40:53.4273349Z",
              "session_id": "29",
              "session_start_time": "2025-03-21T11:40:53.428703Z",
              "spark_jobs": null,
              "spark_pool": "ecomsparkpool",
              "state": "finished",
              "statement_id": 2,
              "statement_ids": [
                2
              ]
            },
            "text/plain": [
              "StatementMeta(ecomsparkpool, 29, 2, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import IntegerType, BooleanType, StringType\n",
        "from pyspark.sql.functions import col, date_format, year, month, dayofmonth, quarter, weekofyear, dayofweek, when, date_add, expr, lit, current_timestamp, monotonically_increasing_id, date_format\n",
        "from pyspark.sql.functions import col, current_timestamp, count, countDistinct, sum as _sum, avg, when, hash as _hash\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Define function for generating ADLS paths (reusing from your existing code)\n",
        "def get_adls_path(container: str, folder: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate an ADLS path based on the container and folder.\n",
        "    \"\"\"\n",
        "    storage_account = \"ecomsalessa\"\n",
        "    return f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{folder}/\"\n",
        "\n",
        "# Define source and destination paths\n",
        "processed_container = \"processed\"\n",
        "processed_folder = \"ecommerce-dataset-l0\"\n",
        "processed_path = get_adls_path(processed_container, processed_folder)\n",
        "\n",
        "curated_container = \"curated\"\n",
        "curated_folder = \"ecommerce-dataset-l1\"\n",
        "curated_path = get_adls_path(curated_container, curated_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading from Processed Layer\n",
        "\n",
        "This section loads the processed datasets from the previous pipeline stage. These datasets have been:\n",
        "1. Cleaned and standardized\n",
        "2. Enhanced with derived fields\n",
        "3. Enriched with English product category translations\n",
        "4. Optimized for analytical processing\n",
        "\n",
        "Data verification ensures all required fields and translations are present before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-21T11:42:47.1500435Z",
              "execution_start_time": "2025-03-21T11:42:20.9236187Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "86c83dde-4ed5-4dce-83e8-bd847b900b02",
              "queued_time": "2025-03-21T11:40:53.4281971Z",
              "session_id": "29",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ecomsparkpool",
              "state": "finished",
              "statement_id": 3,
              "statement_ids": [
                3
              ]
            },
            "text/plain": [
              "StatementMeta(ecomsparkpool, 29, 3, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading processed datasets...\n",
            "Verifying loaded datasets...\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- customer_unique_id: string (nullable = true)\n",
            " |-- customer_zip_code_prefix: integer (nullable = true)\n",
            " |-- customer_city: string (nullable = true)\n",
            " |-- customer_state: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- order_status: string (nullable = true)\n",
            " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
            " |-- order_approved_at: timestamp (nullable = true)\n",
            " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
            " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
            " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
            " |-- shipping_time_days: integer (nullable = true)\n",
            " |-- delivery_time_days: integer (nullable = true)\n",
            " |-- total_delivery_time_days: integer (nullable = true)\n",
            " |-- is_delayed: boolean (nullable = true)\n",
            " |-- delay_days: integer (nullable = true)\n",
            " |-- is_approved: boolean (nullable = true)\n",
            " |-- is_shipped: boolean (nullable = true)\n",
            " |-- is_delivered: boolean (nullable = true)\n",
            " |-- shipping_days: integer (nullable = true)\n",
            " |-- delivery_days: integer (nullable = true)\n",
            " |-- total_days: integer (nullable = true)\n",
            "\n",
            "\n",
            "Verifying product categories:\n",
            "+---------------------+-----------------------------+\n",
            "|product_category_name|product_category_name_english|\n",
            "+---------------------+-----------------------------+\n",
            "|           perfumaria|                    perfumery|\n",
            "|                artes|                          art|\n",
            "|        esporte_lazer|               sports_leisure|\n",
            "|                bebes|                         baby|\n",
            "| utilidades_domest...|                   housewares|\n",
            "+---------------------+-----------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Products with English categories: 32951\n",
            "Products without English categories: 0\n"
          ]
        }
      ],
      "source": [
        "# Load processed datasets\n",
        "print(\"Loading processed datasets...\")\n",
        "customers_df = spark.read.parquet(processed_path + \"customers/\")\n",
        "orders_df = spark.read.parquet(processed_path + \"orders/\")\n",
        "order_items_df = spark.read.parquet(processed_path + \"order_items/\")\n",
        "order_payments_df = spark.read.parquet(processed_path + \"order_payments/\")\n",
        "order_reviews_df = spark.read.parquet(processed_path + \"order_reviews/\")\n",
        "products_df = spark.read.parquet(processed_path + \"products/\")\n",
        "sellers_df = spark.read.parquet(processed_path + \"sellers/\")\n",
        "category_names_df = spark.read.parquet(processed_path + \"category_names/\")\n",
        "geolocation_df = spark.read.parquet(processed_path + \"geolocation/\")\n",
        "\n",
        "# Display schemas to verify data loading\n",
        "print(\"Verifying loaded datasets...\")\n",
        "customers_df.printSchema()\n",
        "orders_df.printSchema()\n",
        "\n",
        "# Verify English category names are available\n",
        "print(\"\\nVerifying product categories:\")\n",
        "products_df.select(\"product_category_name\", \"product_category_name_english\").show(5)\n",
        "print(f\"Products with English categories: {products_df.filter(F.col('product_category_name_english').isNotNull()).count()}\")\n",
        "print(f\"Products without English categories: {products_df.filter(F.col('product_category_name_english').isNull()).count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimension Tables Creation\n",
        "\n",
        "This section implements the dimensional model by creating standardized dimension tables:\n",
        "\n",
        "1. **Customer Dimension**: Geographic and identification attributes of customers\n",
        "2. **Product Dimension**: Product attributes including categories and physical dimensions\n",
        "3. **Seller Dimension**: Seller attributes and location information\n",
        "4. **Date Dimension**: A complete calendar with date hierarchies and attributes\n",
        "5. **Geography Dimension**: Location data for spatial analysis\n",
        "\n",
        "These dimension tables form the reference points for the star schema design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-21T11:42:55.0742411Z",
              "execution_start_time": "2025-03-21T11:42:47.162012Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "9ebd8795-48f5-449a-8179-9bbd013bcec4",
              "queued_time": "2025-03-21T11:40:53.4290258Z",
              "session_id": "29",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ecomsparkpool",
              "state": "finished",
              "statement_id": 4,
              "statement_ids": [
                4
              ]
            },
            "text/plain": [
              "StatementMeta(ecomsparkpool, 29, 4, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dimension tables...\n"
          ]
        }
      ],
      "source": [
        "# 1. CREATE DIMENSION TABLES\n",
        "print(\"Creating dimension tables...\")\n",
        "\n",
        "# 1.1 Customer Dimension\n",
        "dim_customer = customers_df.select(\n",
        "    F.col(\"customer_id\").alias(\"CustomerID\"),\n",
        "    F.col(\"customer_unique_id\").alias(\"CustomerUniqueID\"),\n",
        "    F.col(\"customer_zip_code_prefix\").alias(\"CustomerZipCodePrefix\"),\n",
        "    F.col(\"customer_city\").alias(\"CustomerCity\"),\n",
        "    F.col(\"customer_state\").alias(\"CustomerState\")\n",
        ").withColumn(\"RowEffectiveDate\", F.current_timestamp()\n",
        ").withColumn(\"RowExpirationDate\", F.lit(None).cast(\"timestamp\")\n",
        ").withColumn(\"CurrentFlag\", F.lit(True).cast(\"boolean\"))\n",
        "\n",
        "# 1.2 Product Dimension\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# 1.2 Product Dimension\n",
        "dim_product = products_df.join(\n",
        "    category_names_df,\n",
        "    products_df.product_category_name == category_names_df.product_category_name,\n",
        "    \"left\"\n",
        ").select(\n",
        "    products_df.product_id.alias(\"ProductID\"),\n",
        "    products_df.product_category_name.alias(\"ProductCategoryName\"),\n",
        "    F.coalesce(category_names_df.product_category_name_english, F.lit(\"uncategorized\")).alias(\"ProductCategoryNameEnglish\"),\n",
        "    \n",
        "    F.col(\"product_weight_g\").cast(FloatType()).alias(\"ProductWeightG\"),\n",
        "    F.col(\"product_length_cm\").cast(FloatType()).alias(\"ProductLengthCm\"),\n",
        "    F.col(\"product_height_cm\").cast(FloatType()).alias(\"ProductHeightCm\"),\n",
        "    F.col(\"product_width_cm\").cast(FloatType()).alias(\"ProductWidthCm\"),\n",
        "    \n",
        "    (F.col(\"product_length_cm\").cast(FloatType()) *\n",
        "     F.col(\"product_height_cm\").cast(FloatType()) *\n",
        "     F.col(\"product_width_cm\").cast(FloatType())).alias(\"ProductVolumeCm3\"),\n",
        "\n",
        "    F.when(\n",
        "        products_df.product_length_cm.isNull() |\n",
        "        products_df.product_height_cm.isNull() |\n",
        "        products_df.product_width_cm.isNull(), \"Unknown\"\n",
        "    ).otherwise(\n",
        "        F.when((products_df.product_length_cm.cast(FloatType()) *\n",
        "                products_df.product_height_cm.cast(FloatType()) *\n",
        "                products_df.product_width_cm.cast(FloatType())) < 1000, \"Small\")\n",
        "         .when((products_df.product_length_cm.cast(FloatType()) *\n",
        "                products_df.product_height_cm.cast(FloatType()) *\n",
        "                products_df.product_width_cm.cast(FloatType())) < 8000, \"Medium\")\n",
        "         .otherwise(\"Large\")\n",
        "    ).alias(\"SizeCategory\")\n",
        ").withColumn(\"RowEffectiveDate\", F.current_timestamp()\n",
        ").withColumn(\"RowExpirationDate\", F.lit(None).cast(\"timestamp\")\n",
        ").withColumn(\"CurrentFlag\", F.lit(True).cast(\"boolean\"))\n",
        "\n",
        "# 1.3 Seller Dimension\n",
        "dim_seller = sellers_df.select(\n",
        "    F.col(\"seller_id\").alias(\"SellerID\"),\n",
        "    F.col(\"seller_zip_code_prefix\").alias(\"SellerZipCodePrefix\"),\n",
        "    F.col(\"seller_city\").alias(\"SellerCity\"),\n",
        "    F.col(\"seller_state\").alias(\"SellerState\")\n",
        ").withColumn(\"RowEffectiveDate\", F.current_timestamp()\n",
        ").withColumn(\"RowExpirationDate\", F.lit(None).cast(\"timestamp\")\n",
        ").withColumn(\"CurrentFlag\", F.lit(True).cast(\"boolean\"))\n",
        "\n",
        "# 1.4 Date Dimension\n",
        "# Generate date range from min to max order date\n",
        "min_date = orders_df.agg(F.min(\"order_purchase_timestamp\").cast(\"date\")).collect()[0][0]\n",
        "max_date = orders_df.agg(F.max(\"order_purchase_timestamp\").cast(\"date\")).collect()[0][0]\n",
        "\n",
        "date_range = spark.sql(f\"\"\"\n",
        "SELECT explode(sequence(to_date('{min_date}'), to_date('{max_date}'), interval 1 day)) as date\n",
        "\"\"\")\n",
        "\n",
        "dim_date = date_range.select(\n",
        "    F.date_format(\"date\", \"yyyyMMdd\").cast(IntegerType()).alias(\"DateKey\"),\n",
        "    F.date_format(\"date\", \"yyyy-MM-dd\").alias(\"DateID\"),\n",
        "    F.col(\"date\").alias(\"Date\"),\n",
        "    year(\"date\").alias(\"Year\"),\n",
        "    month(\"date\").alias(\"Month\"),\n",
        "    dayofmonth(\"date\").alias(\"Day\"),\n",
        "    quarter(\"date\").alias(\"Quarter\"),\n",
        "    weekofyear(\"date\").alias(\"WeekOfYear\"),\n",
        "    dayofweek(\"date\").alias(\"DayOfWeek\"),\n",
        "    when(dayofweek(\"date\").isin(1, 7), True).otherwise(False).cast(BooleanType()).alias(\"IsWeekend\"),\n",
        "    date_format(\"date\", \"MMMM\").alias(\"MonthName\"),\n",
        "    date_format(\"date\", \"EEEE\").alias(\"DayName\"),\n",
        "    when(month(\"date\") > 6, year(\"date\") + 1).otherwise(year(\"date\")).alias(\"FiscalYear\"),\n",
        "    when(month(\"date\").between(7, 9), 1)\n",
        "     .when(month(\"date\").between(10, 12), 2)\n",
        "     .when(month(\"date\").between(1, 3), 3)\n",
        "     .otherwise(4).alias(\"FiscalQuarter\"),\n",
        "    lit(None).cast(StringType()).alias(\"Holiday\"),\n",
        "    lit(False).cast(BooleanType()).alias(\"IsHoliday\")\n",
        ")\n",
        "\n",
        "# 1.5 Geography Dimension\n",
        "# Aggregate geolocation data to avoid duplicates\n",
        "dim_geography = geolocation_df.groupBy(\"geolocation_zip_code_prefix\").agg(\n",
        "    F.first(\"geolocation_city\").alias(\"City\"),\n",
        "    F.first(\"geolocation_state\").alias(\"State\"),\n",
        "    F.avg(\"geolocation_lat\").alias(\"Latitude\"),\n",
        "    F.avg(\"geolocation_lng\").alias(\"Longitude\")\n",
        ").withColumnRenamed(\"geolocation_zip_code_prefix\", \"ZipCodePrefix\") \\\n",
        " .withColumn(\"Region\", F.lit(None).cast(\"string\")) \\\n",
        " .withColumn(\"RowEffectiveDate\", F.current_timestamp()) \\\n",
        " .withColumn(\"RowExpirationDate\", F.lit(None).cast(\"timestamp\")) \\\n",
        " .withColumn(\"CurrentFlag\", F.lit(True).cast(\"boolean\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fact Tables Creation\n",
        "\n",
        "This section builds the central fact tables that contain measures and foreign keys to dimensions:\n",
        "\n",
        "1. **Sales Fact**: Transaction-level sales data with order details and performance metrics\n",
        "2. **Reviews Fact**: Customer feedback data with response time analysis\n",
        "\n",
        "These fact tables contain the quantitative metrics that will be analyzed across various dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-21T11:42:55.694731Z",
              "execution_start_time": "2025-03-21T11:42:55.0874929Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "7ab8d148-1aa6-4d2b-a234-f00efe23188f",
              "queued_time": "2025-03-21T11:40:53.4315429Z",
              "session_id": "29",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ecomsparkpool",
              "state": "finished",
              "statement_id": 5,
              "statement_ids": [
                5
              ]
            },
            "text/plain": [
              "StatementMeta(ecomsparkpool, 29, 5, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating fact tables...\n"
          ]
        }
      ],
      "source": [
        "# 2. CREATE FACT TABLES\n",
        "print(\"Creating fact tables...\")\n",
        "\n",
        "fact_sales = order_items_df.join(\n",
        "    orders_df, \"order_id\"\n",
        ").join(\n",
        "    order_payments_df.groupBy(\"order_id\").agg(\n",
        "        F.first(\"payment_type\").alias(\"payment_type\")\n",
        "    ), \"order_id\", \"left\"\n",
        ").join(\n",
        "    customers_df.select(\"customer_id\", \"customer_zip_code_prefix\"), \"customer_id\", \"left\"\n",
        ").join(\n",
        "    sellers_df.select(\"seller_id\", \"seller_state\"), \"seller_id\", \"left\"\n",
        ").withColumn(\n",
        "    \"IsCrossState\", F.when(F.col(\"customer_state\") != F.col(\"seller_state\"), True).otherwise(False).cast(BooleanType())\n",
        ").select(\n",
        "    F.col(\"order_id\").alias(\"OrderID\"),\n",
        "    F.col(\"order_item_id\").alias(\"OrderItemID\"),\n",
        "    F.col(\"customer_id\").alias(\"CustomerID\"),\n",
        "    F.col(\"product_id\").alias(\"ProductID\"),\n",
        "    F.col(\"seller_id\").alias(\"SellerID\"),\n",
        "    F.date_format(\"order_purchase_timestamp\", \"yyyyMMdd\").cast(IntegerType()).alias(\"DateKey\"),\n",
        "    F.col(\"order_status\").alias(\"StatusID\"),\n",
        "    F.col(\"customer_zip_code_prefix\").alias(\"ZipCodePrefix\"),\n",
        "    F.col(\"order_purchase_timestamp\").alias(\"OrderPurchaseTimestamp\"),\n",
        "    F.col(\"order_delivered_customer_date\").alias(\"OrderDeliveredCustomerDate\"),\n",
        "    F.col(\"price\").alias(\"Price\"),\n",
        "    F.col(\"freight_value\").alias(\"FreightValue\"),\n",
        "    (F.col(\"price\") + F.col(\"freight_value\")).alias(\"TotalItemValue\"),\n",
        "    F.col(\"shipping_days\").alias(\"ShippingDays\"),\n",
        "    F.col(\"delivery_days\").alias(\"DeliveryDays\"),\n",
        "    F.col(\"total_days\").alias(\"TotalDays\"),\n",
        "    F.col(\"is_delayed\").cast(BooleanType()).alias(\"IsDelayed\"),\n",
        "    F.col(\"delay_days\").alias(\"DelayDays\"),\n",
        "    F.col(\"IsCrossState\"),\n",
        "    F.col(\"payment_type\").alias(\"PaymentType\")\n",
        ")\n",
        "\n",
        "# 2.2 Customer Reviews Fact Table\n",
        "fact_reviews = order_reviews_df.join(\n",
        "    orders_df.select(\"order_id\", \"customer_id\", \"order_purchase_timestamp\"), \"order_id\"\n",
        ").select(\n",
        "    F.col(\"order_id\").alias(\"OrderID\"),\n",
        "    F.col(\"review_id\").alias(\"ReviewID\"),\n",
        "    F.col(\"customer_id\").alias(\"CustomerID\"),\n",
        "    F.date_format(\"order_purchase_timestamp\", \"yyyyMMdd\").cast(IntegerType()).alias(\"DateKey\"),\n",
        "    F.col(\"review_score\").alias(\"ReviewScore\"),\n",
        "    F.col(\"review_comment_message\").alias(\"ReviewCommentMessage\"),\n",
        "    F.col(\"review_creation_date\").alias(\"ReviewCreationDate\"),\n",
        "    F.col(\"review_answer_timestamp\").alias(\"ReviewAnswerTimestamp\"),\n",
        "    F.datediff(\"review_answer_timestamp\", \"review_creation_date\").alias(\"ReviewResponseDays\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Business Aggregations Creation\n",
        "\n",
        "This section generates pre-calculated aggregations for common business analytics needs:\n",
        "\n",
        "1. **Sales by Category**: Product category performance metrics\n",
        "2. **Sales by State**: Geographic sales distribution analysis\n",
        "3. **Seller Performance**: Seller efficiency and delivery metrics\n",
        "4. **Monthly Sales Trends**: Time-series analysis of sales patterns\n",
        "5. **Order Status Analysis**: Order fulfillment and cancellation analysis\n",
        "6. **Cross-State Analysis**: Performance metrics for orders shipped across state borders\n",
        "7. **Product Size Analysis**: Impact of physical dimensions on sales and shipping\n",
        "8. **Payment Method Analysis**: Payment preference patterns\n",
        "\n",
        "Each aggregation includes relevant counts, sums, and averages to support business decision-making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-21T11:43:37.2227148Z",
              "execution_start_time": "2025-03-21T11:42:55.7082599Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "385906a8-9fd3-4d32-918c-52d3307d4093",
              "queued_time": "2025-03-21T11:40:53.4322124Z",
              "session_id": "29",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ecomsparkpool",
              "state": "finished",
              "statement_id": 6,
              "statement_ids": [
                6
              ]
            },
            "text/plain": [
              "StatementMeta(ecomsparkpool, 29, 6, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating business aggregations...\n",
            "Creating order status analysis...\n",
            "Creating product size analysis...\n",
            "Creating payment method analysis...\n",
            "\n",
            "Sample data from agg_payment_methods:\n",
            "+------------+------------+--------------------+------------------+----------------+\n",
            "|payment_type|orders_count|         total_sales|   avg_order_value|unique_customers|\n",
            "+------------+------------+--------------------+------------------+----------------+\n",
            "|      boleto|       22867|  2391525.6600000267| 104.5841457121628|           19614|\n",
            "| credit_card|       86769|1.0974357299999023E7|126.47785845173993|           75991|\n",
            "|     voucher|        6274|   659473.6399999988|105.11215173732847|            3766|\n",
            "|  debit_card|        1691|  183758.74000000037|108.66868125369625|            1521|\n",
            "+------------+------------+--------------------+------------------+----------------+\n",
            "\n",
            "\n",
            "Data quality checks:\n",
            "Order status analysis record count: 190\n",
            "Cross-state analysis record count: 143\n",
            "Size analysis record count: 194\n",
            "\n",
            "Null values in key metrics:\n",
            "Order status analysis nulls in total_sales: 0\n",
            "\n",
            "Actual column names in agg_cross_state:\n",
            "['product_category_name_english', 'is_cross_state', 'orders_count', 'total_sales', 'avg_delivery_time', 'delay_rate']\n",
            "Cross-state analysis nulls in avg_delivery_time: 0\n"
          ]
        }
      ],
      "source": [
        "# 3. CREATE BUSINESS AGGREGATIONS\n",
        "print(\"Creating business aggregations...\")\n",
        "\n",
        "# 3.1 Sales by Category\n",
        "agg_sales_by_category = fact_sales.join(\n",
        "    dim_product, \"product_id\"\n",
        ").groupBy(\n",
        "    col(\"product_category_name_english\").alias(\"CategoryKey\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    countDistinct(\"customer_id\").alias(\"UniqueCustomers\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    avg(\"price\").alias(\"AvgItemPrice\"),\n",
        "    avg(\"total_days\").alias(\"AvgDeliveryTime\"),\n",
        "    _sum(when(col(\"is_delayed\") == True, 1).otherwise(0)).alias(\"DelayedOrders\")\n",
        ").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# 3.2 Sales by State\n",
        "agg_sales_by_state = fact_sales.join(\n",
        "    dim_customer, \"customer_id\"\n",
        ").groupBy(\n",
        "    col(\"customer_state\").alias(\"CustomerState\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    countDistinct(\"customer_id\").alias(\"UniqueCustomers\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    avg(\"price\").alias(\"AvgItemPrice\"),\n",
        "    avg(\"total_days\").alias(\"AvgDeliveryTime\")\n",
        ").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# 3.3 Seller Performance\n",
        "agg_seller_performance = fact_sales.join(\n",
        "    dim_seller, \"seller_id\"\n",
        ").groupBy(\n",
        "    col(\"seller_id\").alias(\"SellerID\"),\n",
        "    col(\"seller_state\").alias(\"SellerState\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    avg(\"freight_value\").alias(\"AvgShippingCost\"),\n",
        "    avg(\"total_days\").alias(\"AvgDeliveryTime\"),\n",
        "    _sum(when(col(\"is_delayed\") == True, 1).otherwise(0)).alias(\"DelayedOrders\"),\n",
        "    ( _sum(when(col(\"is_delayed\") == True, 1).otherwise(0)) / count(\"order_id\") ).alias(\"DelayRate\")\n",
        ").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# 3.4 Monthly Sales Trends\n",
        "agg_monthly_sales = fact_sales.join(\n",
        "    dim_date, \"date_id\"\n",
        ").groupBy(\n",
        "    col(\"year\").alias(\"Year\"),\n",
        "    col(\"month\").alias(\"Month\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    countDistinct(\"customer_id\").alias(\"UniqueCustomers\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    avg(\"price\").alias(\"AvgItemPrice\")\n",
        ").orderBy(\"Year\", \"Month\").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# 3.5 Order Status Analysis\n",
        "# (If you wish to persist this aggregation, adjust the table schema accordingly)\n",
        "agg_order_status = fact_sales.join(\n",
        "    dim_product, \"product_id\"\n",
        ").groupBy(\n",
        "    col(\"product_category_name_english\").alias(\"CategoryKey\"),\n",
        "    col(\"order_status\").alias(\"OrderStatus\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    countDistinct(\"customer_id\").alias(\"UniqueCustomers\")\n",
        ").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# 3.6 Cross-State Order Analysis\n",
        "from pyspark.sql.functions import expr\n",
        "agg_cross_state = fact_sales.join(\n",
        "    dim_customer, \"customer_id\"\n",
        ").join(\n",
        "    dim_seller, \"seller_id\"\n",
        ").join(\n",
        "    dim_product, \"product_id\"\n",
        ").withColumn(\n",
        "    \"IsCrossState\", when(col(\"customer_state\") != col(\"seller_state\"), True).otherwise(False).cast(BooleanType())\n",
        ").groupBy(\n",
        "    col(\"product_category_name_english\").alias(\"CategoryKey\"),\n",
        "    col(\"IsCrossState\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    avg(\"total_days\").alias(\"AvgDeliveryTime\"),\n",
        "    ( _sum(when(col(\"is_delayed\") == True, 1).otherwise(0)) / count(\"order_id\") ).alias(\"DelayRate\")\n",
        ").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# 3.7 Product Size Analysis\n",
        "# Note: dim_product_with_size is assumed to be derived from dim_product with proper casting.\n",
        "agg_size_analysis = fact_sales.join(\n",
        "    dim_product_with_size, \"product_id\"\n",
        ").groupBy(\n",
        "    col(\"product_category_name_english\").alias(\"CategoryKey\"),\n",
        "    col(\"size_category\").alias(\"SizeCategory\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    avg(\"freight_value\").alias(\"AvgShippingCost\")\n",
        ").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# 3.8 Payment Method Analysis\n",
        "agg_payment_methods = fact_sales.join(\n",
        "    order_payments_df.select(\"order_id\", \"payment_type\"),\n",
        "    \"order_id\"\n",
        ").groupBy(\n",
        "    _hash(\"payment_type\").alias(\"PaymentTypeKey\")\n",
        ").agg(\n",
        "    count(\"order_id\").alias(\"OrdersCount\"),\n",
        "    _sum(\"price\").alias(\"TotalSales\"),\n",
        "    avg(\"price\").alias(\"AvgOrderValue\"),\n",
        "    countDistinct(\"customer_id\").alias(\"UniqueCustomers\")\n",
        ").withColumn(\"LastUpdated\", current_timestamp())\n",
        "\n",
        "# Write out payment method aggregation as an example:\n",
        "agg_payment_methods.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/payment_methods/\")\n",
        "\n",
        "# Optionally, display some sample data:\n",
        "print(\"\\nSample data from agg_payment_methods:\")\n",
        "agg_payment_methods.show(5)\n",
        "\n",
        "# Data quality checks\n",
        "print(\"\\nData quality checks:\")\n",
        "print(f\"Order status analysis record count: {agg_order_status.count()}\")\n",
        "print(f\"Cross-state analysis record count: {agg_cross_state.count()}\")\n",
        "print(f\"Size analysis record count: {agg_size_analysis.count()}\")\n",
        "\n",
        "print(\"\\nNull values in key metrics:\")\n",
        "print(\"Order status analysis nulls in TotalSales:\", \n",
        "      agg_order_status.filter(col(\"TotalSales\").isNull()).count())\n",
        "\n",
        "print(\"\\nActual column names in agg_cross_state:\")\n",
        "print(agg_cross_state.columns)\n",
        "\n",
        "print(\"Cross-state analysis nulls in AvgDeliveryTime:\", \n",
        "      agg_cross_state.filter(col(\"AvgDeliveryTime\").isNull()).count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Persistence and Verification\n",
        "\n",
        "This section:\n",
        "1. Writes all dimension and fact tables to the curated layer\n",
        "2. Persists aggregated datasets for quick access by reporting tools\n",
        "3. Displays sample data to verify the output quality\n",
        "4. Provides summary statistics on the processed data volume\n",
        "\n",
        "The curated data is now optimized for consumption by business intelligence tools and dashboards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-21T11:45:09.3602088Z",
              "execution_start_time": "2025-03-21T11:43:37.2366182Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "97d1dd45-93c4-417a-adea-ac16dbd5ded4",
              "queued_time": "2025-03-21T11:40:53.4328233Z",
              "session_id": "29",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ecomsparkpool",
              "state": "finished",
              "statement_id": 7,
              "statement_ids": [
                7
              ]
            },
            "text/plain": [
              "StatementMeta(ecomsparkpool, 29, 7, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing dimension tables to curated container...\n",
            "Writing fact tables to curated container...\n",
            "Writing aggregation tables to curated container...\n",
            "\n",
            "Writing aggregations to curated layer...\n",
            "\n",
            "Sample data from fact_sales:\n",
            "+--------------------+--------------------+-----+----------+----------+\n",
            "|            order_id|         customer_id|price|total_days|is_delayed|\n",
            "+--------------------+--------------------+-----+----------+----------+\n",
            "|995392413cee61cc1...|4bf24904ec428325a...|129.9|        18|     false|\n",
            "|b39de9ed2bb8fd08a...|ed18b557140ff674f...| 98.9|        15|     false|\n",
            "|b1a88554eb1f7f686...|5cf799d0ac88e1d32...|39.99|         7|     false|\n",
            "|fe2ac30768e07e362...|be07f06f183227b5c...| 74.5|        10|     false|\n",
            "|460673777918e7a42...|6f352988122e277e6...| 48.0|        17|     false|\n",
            "+--------------------+--------------------+-----+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Sample data from agg_sales_by_category:\n",
            "+-----------------------------+------------+----------------+------------------+------------------+------------------+--------------+\n",
            "|product_category_name_english|orders_count|unique_customers|       total_sales|    avg_item_price| avg_delivery_time|delayed_orders|\n",
            "+-----------------------------+------------+----------------+------------------+------------------+------------------+--------------+\n",
            "|                uncategorized|        1627|            1473| 185049.7600000004| 113.7367916410574|12.105101413644745|           145|\n",
            "|                          art|         209|             202|          24202.64| 115.8021052631579|10.598086124401913|            15|\n",
            "|                      flowers|          33|              29|           1110.04| 33.63757575757575|11.121212121212121|             1|\n",
            "|            home_construction|         604|             490|          83088.12|137.56311258278146| 12.96523178807947|            49|\n",
            "|         fashion_male_clot...|         132|             112|10797.820000000002| 81.80166666666668|12.159090909090908|             7|\n",
            "+-----------------------------+------------+----------------+------------------+------------------+------------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Sample data from agg_monthly_sales:\n",
            "+----+-----+------------+----------------+------------------+------------------+\n",
            "|year|month|orders_count|unique_customers|       total_sales|    avg_item_price|\n",
            "+----+-----+------------+----------------+------------------+------------------+\n",
            "|2016|    9|           6|               3|            267.36|             44.56|\n",
            "|2016|   10|         363|             308| 49507.65999999996|136.38473829201092|\n",
            "|2016|   12|           1|               1|              10.9|              10.9|\n",
            "|2017|    1|         955|             789|120312.87000000016|125.98206282722529|\n",
            "|2017|    2|        1951|            1733| 247303.0200000006|126.75705791901619|\n",
            "+----+-----+------------+----------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Sample data from agg_order_status:\n",
            "+-----------------------------+------------+------------+-----------------+----------------+\n",
            "|product_category_name_english|order_status|orders_count|      total_sales|unique_customers|\n",
            "+-----------------------------+------------+------------+-----------------+----------------+\n",
            "|             office_furniture|   DELIVERED|        1668|268154.3100000001|            1254|\n",
            "|               consoles_games|     SHIPPED|           6|618.4100000000001|               6|\n",
            "|            furniture_bedroom|   DELIVERED|         103|          19051.8|              90|\n",
            "|             small_appliances|    APPROVED|          17|          6336.34|              17|\n",
            "|                   food_drink|     SHIPPED|           2|            105.8|               2|\n",
            "+-----------------------------+------------+------------+-----------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Sample data from agg_cross_state:\n",
            "+-----------------------------+--------------+------------+------------------+------------------+-------------------+\n",
            "|product_category_name_english|is_cross_state|orders_count|       total_sales| avg_delivery_time|         delay_rate|\n",
            "+-----------------------------+--------------+------------+------------------+------------------+-------------------+\n",
            "|                  electronics|         false|         859|46072.719999999914| 7.128055878928987|0.05587892898719441|\n",
            "|                       drinks|          true|         197|14079.609999999993|12.756345177664974|0.04060913705583756|\n",
            "|                       drinks|         false|         182|           8349.09|6.6098901098901095|0.06593406593406594|\n",
            "|                         baby|          true|        1978| 276799.3200000004|14.672396359959555|0.09706774519716886|\n",
            "|              fixed_telephony|         false|          99|12393.489999999998| 8.252525252525253|0.04040404040404041|\n",
            "+-----------------------------+--------------+------------+------------------+------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Sample data from agg_size_analysis:\n",
            "+-----------------------------+-------------+------------+------------------+------------------+\n",
            "|product_category_name_english|size_category|orders_count|       total_sales| avg_shipping_cost|\n",
            "+-----------------------------+-------------+------------+------------------+------------------+\n",
            "|         computers_accesso...|        Large|        1460|306049.88999999984| 27.25920547945207|\n",
            "|                     pet_shop|        Large|        1182|143441.18000000034|23.040059221658208|\n",
            "|                fashion_sport|       Medium|          26|1622.9099999999999| 15.23230769230769|\n",
            "|         construction_tool...|        Small|          28|           1483.51|13.916785714285714|\n",
            "|            home_appliances_2|       Medium|          19|1860.9699999999998|18.644736842105264|\n",
            "+-----------------------------+-------------+------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Data warehousing process completed successfully!\n",
            "\n",
            "=== ETL Pipeline Summary ===\n",
            "Processed 112650 sales records\n",
            "Created 32951 product dimension records\n",
            "Created 99441 customer dimension records\n",
            "Created 3095 seller dimension records\n",
            "Generated 3745 analytical aggregations\n",
            "Pipeline execution complete!\n"
          ]
        }
      ],
      "source": [
        "# 4. WRITE TO CURATED CONTAINER\n",
        "print(\"Writing dimension tables to curated container...\")\n",
        "\n",
        "# Write dimensions\n",
        "dim_customer.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_customer/\")\n",
        "dim_product.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_product/\")\n",
        "dim_seller.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_seller/\")\n",
        "dim_date.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_date/\")\n",
        "dim_geography.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_geography/\")\n",
        "\n",
        "print(\"Writing fact tables to curated container...\")\n",
        "# Write facts\n",
        "fact_sales.write.mode(\"overwrite\").parquet(curated_path + \"facts/fact_sales/\")\n",
        "fact_reviews.write.mode(\"overwrite\").parquet(curated_path + \"facts/fact_reviews/\")\n",
        "\n",
        "print(\"Writing aggregation tables to curated container...\")\n",
        "# Write aggregations\n",
        "# Write all aggregations to curated layer\n",
        "print(\"\\nWriting aggregations to curated layer...\")\n",
        "agg_sales_by_category.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/sales_by_category/\")\n",
        "agg_sales_by_state.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/sales_by_state/\")\n",
        "agg_seller_performance.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/seller_performance/\")\n",
        "agg_monthly_sales.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/monthly_sales/\")\n",
        "agg_order_status.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/order_status/\")\n",
        "agg_cross_state.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/cross_state_analysis/\")\n",
        "agg_size_analysis.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/size_analysis/\")\n",
        "\n",
        "# Show sample data from key tables\n",
        "print(\"\\nSample data from fact_sales:\")\n",
        "fact_sales.select(\"order_id\", \"customer_id\", \"price\", \"total_days\", \"is_delayed\").show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_sales_by_category:\")\n",
        "agg_sales_by_category.show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_monthly_sales:\")\n",
        "agg_monthly_sales.show(5)\n",
        "\n",
        "# Add these lines after your existing sample data displays\n",
        "print(\"\\nSample data from agg_order_status:\")\n",
        "agg_order_status.show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_cross_state:\")\n",
        "agg_cross_state.show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_size_analysis:\")\n",
        "agg_size_analysis.show(5)\n",
        "\n",
        "print(\"\\nData warehousing process completed successfully!\")\n",
        "\n",
        "# Final pipeline summary\n",
        "print(\"\\n=== ETL Pipeline Summary ===\")\n",
        "print(f\"Processed {fact_sales.count()} sales records\")\n",
        "print(f\"Created {dim_product.count()} product dimension records\")\n",
        "print(f\"Created {dim_customer.count()} customer dimension records\")\n",
        "print(f\"Created {dim_seller.count()} seller dimension records\")\n",
        "print(f\"Generated {agg_sales_by_category.count() + agg_sales_by_state.count() + agg_seller_performance.count() + agg_monthly_sales.count() + agg_order_status.count() + agg_cross_state.count() + agg_size_analysis.count()} analytical aggregations\")\n",
        "print(\"Pipeline execution complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. WRITE TO CURATED CONTAINER - CSVs in a single folder\n",
        "csv_base_path = curated_path + \"csvs/\"\n",
        "\n",
        "print(\"Writing dimension tables to curated container as CSVs...\")\n",
        "\n",
        "# Write dimensions as CSV with header\n",
        "dim_customer.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"dim_customer/\")\n",
        "dim_product.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"dim_product/\")\n",
        "dim_seller.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"dim_seller/\")\n",
        "dim_date.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"dim_date/\")\n",
        "dim_geography.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"dim_geography/\")\n",
        "\n",
        "print(\"Writing fact tables to curated container as CSVs...\")\n",
        "# Write facts as CSV\n",
        "fact_sales.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"fact_sales/\")\n",
        "fact_reviews.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"fact_reviews/\")\n",
        "\n",
        "print(\"Writing aggregation tables to curated container as CSVs...\")\n",
        "# Write aggregations as CSV\n",
        "agg_sales_by_category.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"agg_sales_by_category/\")\n",
        "agg_sales_by_state.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"agg_sales_by_state/\")\n",
        "agg_seller_performance.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"agg_seller_performance/\")\n",
        "agg_monthly_sales.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"agg_monthly_sales/\")\n",
        "agg_order_status.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"agg_order_status/\")\n",
        "agg_cross_state.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"agg_cross_state_analysis/\")\n",
        "agg_size_analysis.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_base_path + \"agg_size_analysis/\")\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
