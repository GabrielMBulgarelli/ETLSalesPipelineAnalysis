{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E-Commerce Data Curated Analytics - Dimensional Modeling & Aggregation\n",
        "\n",
        "This notebook represents the final stage in the E-commerce ETL pipeline. It:\n",
        "1. Creates a dimensional model (star schema) from processed data\n",
        "2. Builds business aggregations for analytics\n",
        "3. Performs data quality checks\n",
        "4. Writes curated data to the data lake for consumption\n",
        "\n",
        "### Configuration and Environment Setup\n",
        "\n",
        "The following cell initializes the Spark environment and defines storage paths for the ETL process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Define function for generating ADLS paths (reusing from your existing code)\n",
        "def get_adls_path(container: str, folder: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate an ADLS path based on the container and folder.\n",
        "    \"\"\"\n",
        "    storage_account = \"ecomsalessa\"\n",
        "    return f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{folder}/\"\n",
        "\n",
        "# Define source and destination paths\n",
        "processed_container = \"processed\"\n",
        "processed_folder = \"ecommerce-dataset-l0\"\n",
        "processed_path = get_adls_path(processed_container, processed_folder)\n",
        "\n",
        "curated_container = \"curated\"\n",
        "curated_folder = \"ecommerce-dataset-l1\"\n",
        "curated_path = get_adls_path(curated_container, curated_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading from Processed Layer\n",
        "\n",
        "This section loads the processed datasets from the previous pipeline stage. These datasets have been:\n",
        "1. Cleaned and standardized\n",
        "2. Enhanced with derived fields\n",
        "3. Enriched with English product category translations\n",
        "4. Optimized for analytical processing\n",
        "\n",
        "Data verification ensures all required fields and translations are present before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed datasets\n",
        "print(\"Loading processed datasets...\")\n",
        "customers_df = spark.read.parquet(processed_path + \"customers/\")\n",
        "orders_df = spark.read.parquet(processed_path + \"orders/\")\n",
        "order_items_df = spark.read.parquet(processed_path + \"order_items/\")\n",
        "order_payments_df = spark.read.parquet(processed_path + \"order_payments/\")\n",
        "order_reviews_df = spark.read.parquet(processed_path + \"order_reviews/\")\n",
        "products_df = spark.read.parquet(processed_path + \"products/\")\n",
        "sellers_df = spark.read.parquet(processed_path + \"sellers/\")\n",
        "category_names_df = spark.read.parquet(processed_path + \"category_names/\")\n",
        "geolocation_df = spark.read.parquet(processed_path + \"geolocation/\")\n",
        "\n",
        "# Display schemas to verify data loading\n",
        "print(\"Verifying loaded datasets...\")\n",
        "customers_df.printSchema()\n",
        "orders_df.printSchema()\n",
        "\n",
        "# Verify English category names are available\n",
        "print(\"\\nVerifying product categories:\")\n",
        "products_df.select(\"product_category_name\", \"product_category_name_english\").show(5)\n",
        "print(f\"Products with English categories: {products_df.filter(F.col('product_category_name_english').isNotNull()).count()}\")\n",
        "print(f\"Products without English categories: {products_df.filter(F.col('product_category_name_english').isNull()).count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimension Tables Creation\n",
        "\n",
        "This section implements the dimensional model by creating standardized dimension tables:\n",
        "\n",
        "1. **Customer Dimension**: Geographic and identification attributes of customers\n",
        "2. **Product Dimension**: Product attributes including categories and physical dimensions\n",
        "3. **Seller Dimension**: Seller attributes and location information\n",
        "4. **Date Dimension**: A complete calendar with date hierarchies and attributes\n",
        "5. **Geography Dimension**: Location data for spatial analysis\n",
        "\n",
        "These dimension tables form the reference points for the star schema design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. CREATE DIMENSION TABLES\n",
        "print(\"Creating dimension tables...\")\n",
        "\n",
        "# 1.1 Customer Dimension\n",
        "dim_customer = customers_df.select(\n",
        "    \"customer_id\",\n",
        "    \"customer_unique_id\",\n",
        "    \"customer_zip_code_prefix\",\n",
        "    \"customer_city\",\n",
        "    \"customer_state\"\n",
        ")\n",
        "\n",
        "# 1.2 Product Dimension\n",
        "dim_product = products_df.join(\n",
        "    category_names_df,\n",
        "    products_df.product_category_name == category_names_df.product_category_name,\n",
        "    \"left\"\n",
        ").select(\n",
        "    products_df.product_id,\n",
        "    products_df.product_category_name,\n",
        "    F.coalesce(category_names_df.product_category_name_english, F.lit(\"uncategorized\")).alias(\"product_category_name_english\"),\n",
        "    products_df.product_weight_g,\n",
        "    products_df.product_length_cm,\n",
        "    products_df.product_height_cm,\n",
        "    products_df.product_width_cm,\n",
        "    # Calculate volume in cubic cm\n",
        "    (products_df.product_length_cm * products_df.product_height_cm * products_df.product_width_cm).alias(\"product_volume_cm3\")\n",
        ")\n",
        "\n",
        "# 1.3 Seller Dimension\n",
        "dim_seller = sellers_df.select(\n",
        "    \"seller_id\",\n",
        "    \"seller_zip_code_prefix\",\n",
        "    \"seller_city\",\n",
        "    \"seller_state\"\n",
        ")\n",
        "\n",
        "# 1.4 Date Dimension\n",
        "# Get min and max dates from orders\n",
        "min_date = orders_df.agg(F.min(\"order_purchase_timestamp\").cast(\"date\")).collect()[0][0]\n",
        "max_date = orders_df.agg(F.max(\"order_purchase_timestamp\").cast(\"date\")).collect()[0][0]\n",
        "\n",
        "# Create date range\n",
        "date_range = spark.sql(f\"\"\"\n",
        "SELECT explode(sequence(to_date('{min_date}'), to_date('{max_date}'), interval 1 day)) as date\n",
        "\"\"\")\n",
        "\n",
        "# Build date dimension\n",
        "dim_date = date_range.select(\n",
        "    F.col(\"date\").cast(\"string\").alias(\"date_id\"),\n",
        "    F.col(\"date\"),\n",
        "    F.year(\"date\").alias(\"year\"),\n",
        "    F.month(\"date\").alias(\"month\"),\n",
        "    F.dayofmonth(\"date\").alias(\"day\"),\n",
        "    F.quarter(\"date\").alias(\"quarter\"),\n",
        "    F.weekofyear(\"date\").alias(\"week_of_year\"),\n",
        "    F.dayofweek(\"date\").alias(\"day_of_week\"),\n",
        "    F.when(F.dayofweek(\"date\").isin(1, 7), True).otherwise(False).alias(\"is_weekend\")\n",
        ")\n",
        "\n",
        "# 1.5 Geography Dimension\n",
        "# Aggregate geolocation data to avoid duplicates\n",
        "dim_geography = geolocation_df.groupBy(\"geolocation_zip_code_prefix\").agg(\n",
        "    F.first(\"geolocation_city\").alias(\"city\"),\n",
        "    F.first(\"geolocation_state\").alias(\"state\"),\n",
        "    F.avg(\"geolocation_lat\").alias(\"latitude\"),\n",
        "    F.avg(\"geolocation_lng\").alias(\"longitude\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fact Tables Creation\n",
        "\n",
        "This section builds the central fact tables that contain measures and foreign keys to dimensions:\n",
        "\n",
        "1. **Sales Fact**: Transaction-level sales data with order details and performance metrics\n",
        "2. **Reviews Fact**: Customer feedback data with response time analysis\n",
        "\n",
        "These fact tables contain the quantitative metrics that will be analyzed across various dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. CREATE FACT TABLES\n",
        "print(\"Creating fact tables...\")\n",
        "\n",
        "# 2.1 Sales Fact Table\n",
        "fact_sales = order_items_df.join(\n",
        "    orders_df, \"order_id\"\n",
        ").join(\n",
        "    order_payments_df.groupBy(\"order_id\").agg(\n",
        "        F.sum(\"payment_value\").alias(\"total_payment\"),\n",
        "        F.collect_list(\"payment_type\").alias(\"payment_methods\")\n",
        "    ), \"order_id\", \"left\"\n",
        ").select(\n",
        "    order_items_df.order_id,\n",
        "    order_items_df.order_item_id,\n",
        "    orders_df.customer_id,\n",
        "    order_items_df.product_id,\n",
        "    order_items_df.seller_id,\n",
        "    F.date_format(orders_df.order_purchase_timestamp, \"yyyy-MM-dd\").alias(\"date_id\"),\n",
        "    orders_df.order_purchase_timestamp,\n",
        "    orders_df.order_delivered_customer_date,\n",
        "    orders_df.order_status,\n",
        "    orders_df.is_approved,\n",
        "    orders_df.is_delivered,\n",
        "    order_items_df.price,\n",
        "    order_items_df.freight_value,\n",
        "    (order_items_df.price + order_items_df.freight_value).alias(\"total_item_value\"),\n",
        "    orders_df.shipping_days,\n",
        "    orders_df.delivery_days,\n",
        "    orders_df.total_days,\n",
        "    orders_df.is_delayed,\n",
        "    orders_df.delay_days\n",
        ")\n",
        "\n",
        "# 2.2 Customer Reviews Fact Table\n",
        "fact_reviews = order_reviews_df.join(\n",
        "    orders_df.select(\"order_id\", \"customer_id\"), \"order_id\"\n",
        ").select(\n",
        "    \"order_id\",\n",
        "    \"review_id\",\n",
        "    \"customer_id\",\n",
        "    \"review_score\",\n",
        "    \"review_comment_message\",\n",
        "    \"review_creation_date\",\n",
        "    \"review_answer_timestamp\",\n",
        "    F.datediff(\"review_answer_timestamp\", \"review_creation_date\").alias(\"review_response_days\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Business Aggregations Creation\n",
        "\n",
        "This section generates pre-calculated aggregations for common business analytics needs:\n",
        "\n",
        "1. **Sales by Category**: Product category performance metrics\n",
        "2. **Sales by State**: Geographic sales distribution analysis\n",
        "3. **Seller Performance**: Seller efficiency and delivery metrics\n",
        "4. **Monthly Sales Trends**: Time-series analysis of sales patterns\n",
        "5. **Order Status Analysis**: Order fulfillment and cancellation analysis\n",
        "6. **Cross-State Analysis**: Performance metrics for orders shipped across state borders\n",
        "7. **Product Size Analysis**: Impact of physical dimensions on sales and shipping\n",
        "8. **Payment Method Analysis**: Payment preference patterns\n",
        "\n",
        "Each aggregation includes relevant counts, sums, and averages to support business decision-making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. CREATE BUSINESS AGGREGATIONS\n",
        "print(\"Creating business aggregations...\")\n",
        "\n",
        "# 3.1 Sales by Category\n",
        "agg_sales_by_category = fact_sales.join(\n",
        "    dim_product, \"product_id\"\n",
        ").groupBy(\n",
        "    \"product_category_name_english\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.avg(\"price\").alias(\"avg_item_price\"),\n",
        "    F.avg(\"total_days\").alias(\"avg_delivery_time\"),\n",
        "    F.sum(F.when(F.col(\"is_delayed\") == True, 1).otherwise(0)).alias(\"delayed_orders\")\n",
        ")\n",
        "\n",
        "# 3.2 Sales by State\n",
        "agg_sales_by_state = fact_sales.join(\n",
        "    dim_customer, \"customer_id\"\n",
        ").groupBy(\n",
        "    \"customer_state\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.avg(\"price\").alias(\"avg_item_price\"),\n",
        "    F.avg(\"total_days\").alias(\"avg_delivery_time\")\n",
        ")\n",
        "\n",
        "\n",
        "# 3.3 Seller Performance\n",
        "agg_seller_performance = fact_sales.join(\n",
        "    dim_seller, \"seller_id\"\n",
        ").groupBy(\n",
        "    \"seller_id\",\n",
        "    \"seller_state\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.avg(\"freight_value\").alias(\"avg_shipping_cost\"),\n",
        "    F.avg(\"total_days\").alias(\"avg_delivery_time\"),\n",
        "    F.sum(F.when(F.col(\"is_delayed\") == True, 1).otherwise(0)).alias(\"delayed_orders\"),\n",
        "    F.sum(F.when(F.col(\"is_delayed\") == True, 1).otherwise(0)) / F.count(\"order_id\").alias(\"delay_rate\")\n",
        ")\n",
        "\n",
        "# 3.4 Monthly Sales Trends\n",
        "agg_monthly_sales = fact_sales.join(\n",
        "    dim_date, \"date_id\"\n",
        ").groupBy(\n",
        "    \"year\", \n",
        "    \"month\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.avg(\"price\").alias(\"avg_item_price\")\n",
        ").orderBy(\n",
        "    \"year\", \n",
        "    \"month\"\n",
        ")\n",
        "\n",
        "# 3.5 Order Status Analysis (including cancellations)\n",
        "print(\"Creating order status analysis...\")\n",
        "agg_order_status = fact_sales.join(\n",
        "    dim_product, \"product_id\"\n",
        ").groupBy(\n",
        "    \"product_category_name_english\", \"order_status\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.countDistinct(\"customer_id\").alias(\"unique_customers\") \n",
        ")\n",
        "\n",
        "# 3.6 Cross-State Order Analysis \n",
        "agg_cross_state = fact_sales.join(\n",
        "    dim_customer, \"customer_id\"\n",
        ").join(\n",
        "    dim_seller, \"seller_id\"\n",
        ").join(\n",
        "    dim_product, \"product_id\"\n",
        ").withColumn(\n",
        "    \"is_cross_state\", F.when(F.col(\"customer_state\") != F.col(\"seller_state\"), True).otherwise(False)\n",
        ").groupBy(\n",
        "    \"product_category_name_english\", \"is_cross_state\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.avg(\"total_days\").alias(\"avg_delivery_time\"),\n",
        "    # Add extra parentheses around the whole expression\n",
        "    (F.sum(F.when(F.col(\"is_delayed\") == True, 1).otherwise(0)) / F.count(\"order_id\")).alias(\"delay_rate\")\n",
        ")\n",
        "\n",
        "# 3.7 Product Size Analysis\n",
        "print(\"Creating product size analysis...\")\n",
        "dim_product_with_size = dim_product.withColumn(\n",
        "    \"size_category\", \n",
        "    F.when(F.col(\"product_volume_cm3\").isNull(), \"Unknown\")\n",
        "    .otherwise(F.expr(\"case when product_volume_cm3 < 1000 then 'Small' \" + \n",
        "                      \"when product_volume_cm3 < 8000 then 'Medium' \" + \n",
        "                      \"else 'Large' end\"))\n",
        ")\n",
        "\n",
        "agg_size_analysis = fact_sales.join(\n",
        "    dim_product_with_size, \"product_id\"\n",
        ").groupBy(\n",
        "    \"product_category_name_english\", \"size_category\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.avg(\"freight_value\").alias(\"avg_shipping_cost\")\n",
        ")\n",
        "\n",
        "# 3.8 Payment Method Analysis\n",
        "print(\"Creating payment method analysis...\")\n",
        "agg_payment_methods = fact_sales.join(\n",
        "    order_payments_df.select(\"order_id\", \"payment_type\"),\n",
        "    \"order_id\"\n",
        ").groupBy(\n",
        "    \"payment_type\"\n",
        ").agg(\n",
        "    F.count(\"order_id\").alias(\"orders_count\"),\n",
        "    F.sum(\"price\").alias(\"total_sales\"),\n",
        "    F.avg(\"price\").alias(\"avg_order_value\"),\n",
        "    F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
        ")\n",
        "\n",
        "agg_payment_methods.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/payment_methods/\")\n",
        "\n",
        "print(\"\\nSample data from agg_payment_methods:\")\n",
        "agg_payment_methods.show(5)\n",
        "\n",
        "# Data quality verification for key analyses\n",
        "print(\"\\nData quality checks:\")\n",
        "print(f\"Order status analysis record count: {agg_order_status.count()}\")\n",
        "print(f\"Cross-state analysis record count: {agg_cross_state.count()}\")\n",
        "print(f\"Size analysis record count: {agg_size_analysis.count()}\")\n",
        "\n",
        "# Check for null values in key metrics\n",
        "print(\"\\nNull values in key metrics:\")\n",
        "print(\"Order status analysis nulls in total_sales:\", \n",
        "      agg_order_status.filter(F.col(\"total_sales\").isNull()).count())\n",
        "\n",
        "print(\"\\nActual column names in agg_cross_state:\")\n",
        "print(agg_cross_state.columns)\n",
        "\n",
        "print(\"Cross-state analysis nulls in avg_delivery_time:\", \n",
        "      agg_cross_state.filter(F.col(\"avg_delivery_time\").isNull()).count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Persistence and Verification\n",
        "\n",
        "This section:\n",
        "1. Writes all dimension and fact tables to the curated layer\n",
        "2. Persists aggregated datasets for quick access by reporting tools\n",
        "3. Displays sample data to verify the output quality\n",
        "4. Provides summary statistics on the processed data volume\n",
        "\n",
        "The curated data is now optimized for consumption by business intelligence tools and dashboards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. WRITE TO CURATED CONTAINER\n",
        "print(\"Writing dimension tables to curated container...\")\n",
        "\n",
        "# Write dimensions\n",
        "dim_customer.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_customer/\")\n",
        "dim_product.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_product/\")\n",
        "dim_seller.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_seller/\")\n",
        "dim_date.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_date/\")\n",
        "dim_geography.write.mode(\"overwrite\").parquet(curated_path + \"dimensions/dim_geography/\")\n",
        "\n",
        "print(\"Writing fact tables to curated container...\")\n",
        "# Write facts\n",
        "fact_sales.write.mode(\"overwrite\").parquet(curated_path + \"facts/fact_sales/\")\n",
        "fact_reviews.write.mode(\"overwrite\").parquet(curated_path + \"facts/fact_reviews/\")\n",
        "\n",
        "print(\"Writing aggregation tables to curated container...\")\n",
        "# Write aggregations\n",
        "# Write all aggregations to curated layer\n",
        "print(\"\\nWriting aggregations to curated layer...\")\n",
        "agg_sales_by_category.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/sales_by_category/\")\n",
        "agg_sales_by_state.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/sales_by_state/\")\n",
        "agg_seller_performance.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/seller_performance/\")\n",
        "agg_monthly_sales.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/monthly_sales/\")\n",
        "agg_order_status.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/order_status/\")\n",
        "agg_cross_state.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/cross_state_analysis/\")\n",
        "agg_size_analysis.write.mode(\"overwrite\").parquet(curated_path + \"aggregates/size_analysis/\")\n",
        "\n",
        "# Show sample data from key tables\n",
        "print(\"\\nSample data from fact_sales:\")\n",
        "fact_sales.select(\"order_id\", \"customer_id\", \"price\", \"total_days\", \"is_delayed\").show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_sales_by_category:\")\n",
        "agg_sales_by_category.show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_monthly_sales:\")\n",
        "agg_monthly_sales.show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_order_status:\")\n",
        "agg_order_status.show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_cross_state:\")\n",
        "agg_cross_state.show(5)\n",
        "\n",
        "print(\"\\nSample data from agg_size_analysis:\")\n",
        "agg_size_analysis.show(5)\n",
        "\n",
        "print(\"\\nData warehousing process completed successfully!\")\n",
        "\n",
        "# Final pipeline summary\n",
        "print(\"\\n=== ETL Pipeline Summary ===\")\n",
        "print(f\"Processed {fact_sales.count()} sales records\")\n",
        "print(f\"Created {dim_product.count()} product dimension records\")\n",
        "print(f\"Created {dim_customer.count()} customer dimension records\")\n",
        "print(f\"Created {dim_seller.count()} seller dimension records\")\n",
        "print(f\"Generated {agg_sales_by_category.count() + agg_sales_by_state.count() + agg_seller_performance.count() + agg_monthly_sales.count() + agg_order_status.count() + agg_cross_state.count() + agg_size_analysis.count()} analytical aggregations\")\n",
        "print(\"Pipeline execution complete!\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
