{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E-Commerce Data Processing Pipeline\n",
        "### Transformation Layer\n",
        "\n",
        "This notebook represents the second stage in the E-commerce ETL pipeline. It:\n",
        "1. Loads raw Brazilian e-commerce data from the Olist dataset\n",
        "2. Cleans and transforms the data\n",
        "3. Creates derived fields for analysis\n",
        "4. Writes processed data to the processed layer for analytics\n",
        "\n",
        "### Setup and Configuration\n",
        "\n",
        "The code below initializes a Spark session and defines storage paths for reading raw data and writing processed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session (if not already initialized)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Define function for generating ADLS paths\n",
        "def get_adls_path(container: str, folder: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate an ADLS path based on the container and folder.\n",
        "    \"\"\"\n",
        "    storage_account = \"ecomsalessa\"\n",
        "    return f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{folder}/\"\n",
        "\n",
        "# Define source paths (raw data)\n",
        "raw_container = \"raw\"\n",
        "raw_folder = \"ecommerce-dataset\"\n",
        "raw_path = get_adls_path(raw_container, raw_folder)\n",
        "\n",
        "# Define destination paths (processed data)\n",
        "processed_container = \"processed\"\n",
        "processed_folder = \"ecommerce-dataset-l0\"\n",
        "processed_path = get_adls_path(processed_container, processed_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading and Initial Standardization\n",
        "\n",
        "This section performs:\n",
        "1. Loading of raw CSV files from the raw container\n",
        "2. Standardization of column names to snake_case format for consistency\n",
        "3. Implementation of consistent naming conventions across all datasets\n",
        "\n",
        "The Olist dataset includes customers, orders, products, sellers, and their relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Load each dataset from ADLS\n",
        "customers_df = spark.read.csv(raw_path + \"olist_customers_dataset.csv\", header=True, inferSchema=True)\n",
        "orders_df = spark.read.csv(raw_path + \"olist_orders_dataset.csv\", header=True, inferSchema=True)\n",
        "order_items_df = spark.read.csv(raw_path + \"olist_order_items_dataset.csv\", header=True, inferSchema=True)\n",
        "order_payments_df = spark.read.csv(raw_path + \"olist_order_payments_dataset.csv\", header=True, inferSchema=True)\n",
        "order_reviews_df = spark.read.csv(raw_path + \"olist_order_reviews_dataset.csv\", header=True, inferSchema=True)\n",
        "products_df = spark.read.csv(raw_path + \"olist_products_dataset.csv\", header=True, inferSchema=True)\n",
        "sellers_df = spark.read.csv(raw_path + \"olist_sellers_dataset.csv\", header=True, inferSchema=True)\n",
        "category_names_df = spark.read.csv(raw_path + \"product_category_name_translation.csv\", header=True, inferSchema=True)\n",
        "geolocation_df = spark.read.csv(raw_path + \"olist_geolocation_dataset.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Function to convert column names to standardized format (snake_case)\n",
        "def to_snake_case(column_name):\n",
        "    return column_name.lower().replace(' ', '_').replace('-', '_')\n",
        "\n",
        "# Apply to all dataframes\n",
        "customers_df = customers_df.select([F.col(c).alias(to_snake_case(c)) for c in customers_df.columns])\n",
        "orders_df = orders_df.select([F.col(c).alias(to_snake_case(c)) for c in orders_df.columns])\n",
        "order_items_df = order_items_df.select([F.col(c).alias(to_snake_case(c)) for c in order_items_df.columns])\n",
        "order_payments_df = order_payments_df.select([F.col(c).alias(to_snake_case(c)) for c in order_payments_df.columns])\n",
        "order_reviews_df = order_reviews_df.select([F.col(c).alias(to_snake_case(c)) for c in order_reviews_df.columns])\n",
        "products_df = products_df.select([F.col(c).alias(to_snake_case(c)) for c in products_df.columns])\n",
        "sellers_df = sellers_df.select([F.col(c).alias(to_snake_case(c)) for c in sellers_df.columns])\n",
        "category_names_df = category_names_df.select([F.col(c).alias(to_snake_case(c)) for c in category_names_df.columns])\n",
        "geolocation_df = geolocation_df.select([F.col(c).alias(to_snake_case(c)) for c in geolocation_df.columns])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Type Conversion and Null Handling\n",
        "\n",
        "This section addresses data quality issues through:\n",
        "1. Conversion of date/timestamp columns to proper formats\n",
        "2. Conversion of numeric columns to appropriate types\n",
        "3. Handling of null values in critical fields\n",
        "\n",
        "Proper data typing is essential for accurate analysis and reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Convert date columns to proper date format\n",
        "orders_df = orders_df.withColumn(\"order_purchase_timestamp\", F.to_timestamp(\"order_purchase_timestamp\"))\n",
        "orders_df = orders_df.withColumn(\"order_approved_at\", F.to_timestamp(\"order_approved_at\"))\n",
        "orders_df = orders_df.withColumn(\"order_delivered_carrier_date\", F.to_timestamp(\"order_delivered_carrier_date\"))\n",
        "orders_df = orders_df.withColumn(\"order_delivered_customer_date\", F.to_timestamp(\"order_delivered_customer_date\"))\n",
        "orders_df = orders_df.withColumn(\"order_estimated_delivery_date\", F.to_timestamp(\"order_estimated_delivery_date\"))\n",
        "\n",
        "# Convert numeric columns to proper types\n",
        "order_items_df = order_items_df.withColumn(\"price\", F.col(\"price\").cast(DoubleType()))\n",
        "order_items_df = order_items_df.withColumn(\"freight_value\", F.col(\"freight_value\").cast(DoubleType()))\n",
        "order_payments_df = order_payments_df.withColumn(\"payment_value\", F.col(\"payment_value\").cast(DoubleType()))\n",
        "order_reviews_df = order_reviews_df.withColumn(\"review_score\", F.col(\"review_score\").cast(IntegerType()))\n",
        "\n",
        "# Fill null values as needed - use string or value, not None\n",
        "# Instead of None, we'll keep nulls for date columns and use other approaches\n",
        "products_df = products_df.fillna({\"product_category_name\": \"uncategorized\"})\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Exploration and Quality Assessment\n",
        "\n",
        "Before proceeding with transformations, this section:\n",
        "1. Examines the schema of key tables\n",
        "2. Displays sample data to understand structure and content\n",
        "3. Identifies missing values that might impact analysis\n",
        "\n",
        "This assessment reveals data quality issues before dimensional model creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Alternative approach for handling null values in date columns\n",
        "# Use when clause to leave them as null (no change needed)\n",
        "# orders_df = orders_df.withColumn(\"order_approved_at\", \n",
        "#                                 F.when(F.col(\"order_approved_at\").isNotNull(), \n",
        "#                                       F.col(\"order_approved_at\")))\n",
        "\n",
        "# Standardize product categories\n",
        "products_df = products_df.withColumn(\"product_category_name\", F.lower(F.col(\"product_category_name\")))\n",
        "\n",
        "# Display sample data and data info\n",
        "print(\"Customers DataFrame Schema:\")\n",
        "customers_df.printSchema()\n",
        "print(\"\\nCustomers DataFrame Sample:\")\n",
        "customers_df.show(5)\n",
        "\n",
        "print(\"\\nOrders DataFrame Schema:\")\n",
        "orders_df.printSchema()\n",
        "print(\"\\nOrders DataFrame Sample:\")\n",
        "orders_df.show(5)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values in Orders DataFrame:\")\n",
        "for column in orders_df.columns:\n",
        "    missing_count = orders_df.filter(F.col(column).isNull()).count()\n",
        "    if missing_count > 0:\n",
        "        print(f\"Column {column}: {missing_count} missing values\")\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering and Extended Attributes\n",
        "\n",
        "This section enriches the data through:\n",
        "1. Creation of status flags to track order progress\n",
        "2. Calculation of delivery time metrics (shipping days, delivery days)\n",
        "3. Addition of delay indicators and measurements\n",
        "4. Conversion of product categories to English using translation mapping\n",
        "\n",
        "These derived fields enhance analytical capabilities and business insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Add these lines after your date conversion section\n",
        "\n",
        "# Create status flags to track order progress\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"order_status\", \n",
        "    F.when(F.col(\"order_delivered_customer_date\").isNotNull(), \"DELIVERED\")\n",
        "     .when(F.col(\"order_delivered_carrier_date\").isNotNull(), \"SHIPPED\")\n",
        "     .when(F.col(\"order_approved_at\").isNotNull(), \"APPROVED\")\n",
        "     .otherwise(\"CREATED\")\n",
        ")\n",
        "\n",
        "# Calculate delivery time metrics (only where data is available)\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"shipping_time_days\",\n",
        "    F.when(\n",
        "        F.col(\"order_delivered_carrier_date\").isNotNull() & F.col(\"order_approved_at\").isNotNull(),\n",
        "        F.datediff(F.col(\"order_delivered_carrier_date\"), F.col(\"order_approved_at\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"delivery_time_days\",\n",
        "    F.when(\n",
        "        F.col(\"order_delivered_customer_date\").isNotNull() & F.col(\"order_delivered_carrier_date\").isNotNull(),\n",
        "        F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_delivered_carrier_date\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"total_delivery_time_days\",\n",
        "    F.when(\n",
        "        F.col(\"order_delivered_customer_date\").isNotNull() & F.col(\"order_purchase_timestamp\").isNotNull(),\n",
        "        F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_purchase_timestamp\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "# Optional: Add a delivery delay indicator\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"is_delayed\",\n",
        "    F.when(\n",
        "        F.col(\"order_delivered_customer_date\").isNotNull() & F.col(\"order_estimated_delivery_date\").isNotNull(),\n",
        "        F.col(\"order_delivered_customer_date\") > F.col(\"order_estimated_delivery_date\")\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "# Calculate delay days (if delivery was late)\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"delay_days\",\n",
        "    F.when(\n",
        "        F.col(\"is_delayed\") == True,\n",
        "        F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_estimated_delivery_date\"))\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values in Orders DataFrame:\")\n",
        "for column in orders_df.columns:\n",
        "    missing_count = orders_df.filter(F.col(column).isNull()).count()\n",
        "    if missing_count > 0:\n",
        "        print(f\"Column {column}: {missing_count} missing values\")\n",
        "\n",
        "\n",
        "# Add this after your data transformations\n",
        "status_counts = orders_df.groupBy(\"order_status\").count().orderBy(\"order_status\")\n",
        "print(\"\\nOrder Status Distribution:\")\n",
        "status_counts.show()\n",
        "\n",
        "# Add these lines after your existing code\n",
        "\n",
        "# Create additional boolean columns that clearly indicate status\n",
        "orders_df = orders_df.withColumn(\"is_approved\", F.col(\"order_approved_at\").isNotNull())\n",
        "orders_df = orders_df.withColumn(\"is_shipped\", F.col(\"order_delivered_carrier_date\").isNotNull())\n",
        "orders_df = orders_df.withColumn(\"is_delivered\", F.col(\"order_delivered_customer_date\").isNotNull())\n",
        "\n",
        "# Calculate completed times with default values for reporting\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"shipping_days\", \n",
        "    F.when(F.col(\"shipping_time_days\").isNotNull(), F.col(\"shipping_time_days\")).otherwise(-1)\n",
        ")\n",
        "\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"delivery_days\", \n",
        "    F.when(F.col(\"delivery_time_days\").isNotNull(), F.col(\"delivery_time_days\")).otherwise(-1)\n",
        ")\n",
        "\n",
        "orders_df = orders_df.withColumn(\n",
        "    \"total_days\", \n",
        "    F.when(F.col(\"total_delivery_time_days\").isNotNull(), F.col(\"total_delivery_time_days\")).otherwise(-1)\n",
        ")\n",
        "\n",
        "# Add this line to see the new columns\n",
        "print(\"\\nUpdated Orders DataFrame Schema (with boolean flags):\")\n",
        "orders_df.printSchema()\n",
        "\n",
        "# Add this after loading and transforming the datasets, before writing to processed\n",
        "\n",
        "# IMPORTANT: Verify the category translation data\n",
        "print(\"\\nCategory Names DataFrame:\")\n",
        "category_names_df.printSchema()\n",
        "category_names_df.show(5)\n",
        "\n",
        "# Make sure both dataframes have the columns needed for join\n",
        "print(\"\\nProducts DataFrame Category Column:\")\n",
        "products_df.select(\"product_category_name\").show(5)\n",
        "\n",
        "# Fix: Use explicit column references to avoid ambiguity\n",
        "products_with_english_categories = products_df.join(\n",
        "    category_names_df,\n",
        "    products_df[\"product_category_name\"] == category_names_df[\"product_category_name\"],\n",
        "    \"left\"\n",
        ").select(\n",
        "    products_df[\"*\"],  # All columns from products_df\n",
        "    category_names_df[\"product_category_name_english\"]  # Just the English name from category_names_df\n",
        ")\n",
        "\n",
        "# Check the joined data\n",
        "print(\"\\nJoined Products with Categories:\")\n",
        "products_with_english_categories.select(\n",
        "    \"product_id\", \n",
        "    \"product_category_name\", \n",
        "    \"product_category_name_english\"\n",
        ").show(5)\n",
        "\n",
        "# Update the products_df to include the English category names\n",
        "products_df = products_with_english_categories.withColumn(\n",
        "    \"product_category_name_english\",\n",
        "    F.coalesce(F.col(\"product_category_name_english\"), F.lit(\"uncategorized\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Persistence to Processed Layer\n",
        "\n",
        "After all transformations, this section:\n",
        "1. Writes the processed datasets to the processed layer in Parquet format\n",
        "2. Maintains the original entity structure while enhancing with new attributes\n",
        "3. Prepares the data for dimensional modeling in the next pipeline stage\n",
        "\n",
        "The processed data now has consistent formats, enriched attributes, and is optimized for analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Write cleaned data to processed container\n",
        "customers_df.write.mode(\"overwrite\").parquet(processed_path + \"customers/\")\n",
        "orders_df.write.mode(\"overwrite\").parquet(processed_path + \"orders/\")\n",
        "order_items_df.write.mode(\"overwrite\").parquet(processed_path + \"order_items/\")\n",
        "order_payments_df.write.mode(\"overwrite\").parquet(processed_path + \"order_payments/\")\n",
        "order_reviews_df.write.mode(\"overwrite\").parquet(processed_path + \"order_reviews/\")\n",
        "products_df.write.mode(\"overwrite\").parquet(processed_path + \"products/\")\n",
        "sellers_df.write.mode(\"overwrite\").parquet(processed_path + \"sellers/\")\n",
        "category_names_df.write.mode(\"overwrite\").parquet(processed_path + \"category_names/\")\n",
        "geolocation_df.write.mode(\"overwrite\").parquet(processed_path + \"geolocation/\")\n",
        "\n",
        "print(\"Data cleaning and processing completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}